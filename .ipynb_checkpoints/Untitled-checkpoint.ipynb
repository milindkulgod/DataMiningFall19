{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"wordbankdoc.pickle\", \"rb\") as pic:\n",
    "                wordbank = pickle.load(pic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['t', 'r', 'i', 'e', 'd', ' ', 'a', 'n', 'p', 's', 'y', 'c', 'l', 'o', 'm', 'f', 'u', 'x', 'h', 'g', 'z', 'v', 'k', 'w', 'b', 'q', 'j'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordbank.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\makul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import string \n",
    "import html\n",
    "import ast\n",
    "import re\n",
    "import nltk\n",
    "import time\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "stpwrds = stopwords.words(\"english\")\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "dataset = pd.read_csv(\"full1.csv\")\n",
    "\n",
    "def encode_the_reviews(review):\n",
    "        return html.unescape(review)\n",
    "    \n",
    "def elim_stopword(r):\n",
    "        r_n = \" \".join([i for i in r if i not in stpwrds])\n",
    "        return r_n\n",
    "        \n",
    "def lem(tokens):\n",
    "        l = WordNetLemmatizer()\n",
    "        out = [l.lemmatize(word) for word in tokens]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['review']=dataset['review'].apply(str)\n",
    "\n",
    "dataset['review']=dataset['review'].apply(encode_the_reviews)\n",
    "response = dataset['review'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "response = response.apply(lambda r: \" \".join([w for w in r.split() if len(w)>2]))\n",
    "response = [elim_stopword(r.split()) for r in response]\n",
    "response = [r.lower() for r in response]\n",
    "response = pd.Series(response)\n",
    "word_tokens = response.apply(lambda r: r.split())\n",
    "response = word_tokens.apply(lem)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
