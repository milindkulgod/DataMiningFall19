{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\makul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import string \n",
    "import html\n",
    "import ast\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "stpwrds = stopwords.words(\"english\")\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def encode_the_reviews(review):\n",
    "    return html.unescape(review)\n",
    "\n",
    "def elim_stopword(r):\n",
    "    r_n = \" \".join([i for i in r if i not in stpwrds])\n",
    "    return r_n\n",
    "    \n",
    "def lem(tokens):\n",
    "    l = WordNetLemmatizer()\n",
    "    out = [l.lemmatize(word) for word in tokens]\n",
    "    return out\n",
    "\n",
    "def InvInd():\n",
    "dataset = pd.read_csv('rawdata.csv')\n",
    "dataset['review']=dataset['review'].apply(str)\n",
    "dataset['review']=dataset['review'].apply(encode_the_reviews)\n",
    "response = dataset['review'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "response = response.apply(lambda r: \" \".join([w for w in r.split() if len(w)>2]))\n",
    "response = [elim_stopword(r.split()) for r in response]\n",
    "response = [r.lower() for r in response]\n",
    "response = pd.Series(response)\n",
    "word_tokens = response.apply(lambda r: r.split())\n",
    "response = word_tokens.apply(lem)\n",
    "            \n",
    "wordbank = {}\n",
    "            \n",
    "for i,r in enumerate(response, start=0):\n",
    "    for j,w in enumerate(r , start=0):\n",
    "            if w not in wordbank:\n",
    "                wordbank[w] = [1,{i:[j]}]\n",
    "            else:\n",
    "                if i not in wordbank[w][1]:\n",
    "                    wordbank[w][0] += 1\n",
    "                    wordbank[w][1][i] = [j]\n",
    "                else:\n",
    "                    if j not in wordbank[w][1][i]:\n",
    "                        wordbank[w][1][i].append(j)\n",
    "\n",
    "N = np.float64(dataset.shape[0])                    \n",
    "\n",
    "for w in wordbank.keys():\n",
    "    plist = {}\n",
    "    for i in wordbank[w][1].keys():\n",
    "        tf = (len(wordbank[w][1][i])/len(response[i]))\n",
    "        weight_i = (1 + np.log10(tf)) * np.log10(N/wordbank[w][0])\n",
    "        plist[i] = weight_i\n",
    "    wordbank[w].append(plist)\n",
    "p = open('wordbankdoc.pickle',\"wb\")\n",
    "pickle.dump(wordbank,p)\n",
    "\n",
    "\n",
    "def topk(query):\n",
    "    dataset = pd.read_csv('full1.csv', index_col='Unnamed: 0')\n",
    "    p = open('wordbankdoc.pickle',\"rb\")\n",
    "    wordbank = pickle.load(p)\n",
    "\n",
    "    q = query.replace(\"[^a-zA-Z]\", \" \").lower()\n",
    "    q_vec = elim_stopword(q.split())\n",
    "    q_vect = lem(q_vec.split())\n",
    "    \n",
    "    srtdplist = {}\n",
    "    qw = {}\n",
    "    for w in q_vect:\n",
    "        if w in wordbank.keys():\n",
    "            if w not in srtdplist.keys():\n",
    "                srtdplist[w] = sorted(wordbank[w][2].items(), key=lambda x:x[1], reverse=True)[:10]\n",
    "        if w not in qw:\n",
    "            qw[w] = [1,(1/len(q_vect))]\n",
    "        elif w in qw:\n",
    "            qw[w][0] += 1\n",
    "            qw[w][1] = (qw[w][0]/len(q_vect))\n",
    "    if srtdplist == {}:\n",
    "        return \"No results found\"\n",
    "    \n",
    "    topk = []\n",
    "    N = dataset.shape[0]\n",
    "    for i in range(N):\n",
    "        count = 0\n",
    "        sd = 0\n",
    "        for w in q_vect:\n",
    "            for (di,wt) in srtdplist[w]:\n",
    "                if di == i: count += 1\n",
    "        if count > 0 and count == len(q_vect):\n",
    "            for w in q_vect:\n",
    "                l = [x for x in srtdplist[w] if x[0] == i]\n",
    "                sd += l[0][1] * qw[w][1]\n",
    "            topk.append((i,sd))\n",
    "        elif count > 0 and count < len(q_vec):\n",
    "            \n",
    "            for w in q_vect:\n",
    "                l = srtdplist[w][9]\n",
    "                sd += l[1] * qw[w][1]\n",
    "            topk.append((i,sd))  \n",
    "            \n",
    "    show = [x for x in sorted(topk, key=lambda i:i[1], reverse=True)]        \n",
    "    out = []\n",
    "    for (ind,s) in show:\n",
    "         out.append( [dataset.loc[dataset.index[ind], 'drugName'], dataset.loc[dataset.index[ind], 'usefulCount'], dataset.loc[dataset.index[ind], 'condition'], dataset.loc[dataset.index[ind], 'rating'], dataset.loc[dataset.index[ind], 'review'], s*100])\n",
    "    pd.set_option('display.max_columns', -1)  \n",
    "    pd.set_option('display.expand_frame_repr', False)\n",
    "    pd.set_option('max_colwidth', -1)\n",
    " \n",
    "    out =  pd.DataFrame(out, columns=['Drug Name','Useful count','Condition','Rating(/10)','Review','Similarity%'])\n",
    "  \n",
    " \n",
    "    return out  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
